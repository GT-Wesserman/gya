\section{Dirac Notation}
Within quantum mechanics, vectors and lini transformations play an important role. However, as anyone familiar with liniear algebra can tell you, vectors and matricies, which represent linear transformations, can have differet apperances depending on the basis choosen. This, togheter with other needs for a conviniet way to notate vectors and transformations, can be met by using Dirac notation.

\subsection{The Bra, the ket and the inner product}
The ket is, quite simply, a basis independent way to depict a vector, and this vector may be very abstract with an arbitrary number of dimensions which do not neccessarily represent physical space. It is written as $\ket{A}$ and can be expanded as $\ket{A}=A_1\ket{\epsilon_1}+A_2\ket{\epsilon_2}+...+A_N\ket{\epsilon_N}$ where $\ket{\epsilon_i}$ is the ket representing a basis vector of your choosing. However, this is not always necessary, as the kets are do not need a defined basis, but defining one can be useful in different situations, and a new basis can always be used instead. The bra is similair to the ket in appearance, with the bra for the vector $\vec{A}$ looking like $\bra{A}$. The bra itself is however not a vector, but a vector can be "placed" in a bra in order to define it. The bra then becomes a linear functional, which simply means that has the ability to act on a vector to transform it into a scalar. As such, at the relatively simple level we are working at, the bra can be thought of as transforming a vector into a function which acts on other vectors and yields scalar outputs. To see more clearly what this "function" does, we can use column vector notation. Given a vector $\vec{A}$ we get the ket and bra from $\vec{A}$, represented by some basis vectors $\epsilon_1,\epsilon_2,...,\epsilon_N$
\begin{align*}
    \ket{A}&=\begin{pmatrix}
    A_1\\
    A_2\\
    \vdots\\
    A_N
    \end{pmatrix} \\
    \bra{A}&=\begin{pmatrix}
        A_1^* &A_2^* & \dots&A_N^*
    \end{pmatrix}.
\end{align*} By combining the bra with the ket we get the equation
\begin{align*}
    \braket{A}{A}=\begin{pmatrix}
        A_1^* &A_2^* & \dots&A_N^*
    \end{pmatrix} \begin{pmatrix}
         A_1\\
    A_2\\
    \vdots\\
    A_N
    \end{pmatrix}=|\vec{A}|^2.
\end{align*} It is from this definition easy to see that multiplying a bra with a ket is equivilent to the scalar product for real valued 2 or 3 dimensional vectors. This is used to define the inner product, which is simply a more general version of the scalar product. One way to understand what the inner product is and why it is important, we can look to what the scalar product tells us. For two vectors, the scalar product is greater when the vectors are more aligned and less when they are more perpendicular to each other, with the product being 0 when they are completely perpendicular. It is a way to map how much of one vector can be projected in the direction of another. By defining the more general inner product, it becomes possible to see how "parallel" or "perpendicular" more abstract mathematical objects are to each other, as placing objects within kets or bras is not restricted to only vectors. Functions can also be placed within them. There are a few different ways to understand this. One way is to imagine a function to represent a vector with infinetly many components where the index for a basis is any number within the functions domain and the value of the function at any given input is the magnitude of the component with the index of the input. We saw from the example with vectors, and using matrix multiplication, that the inner product of the 2 finite vectors $\vec{A}$ and $\vec{B}$ is \begin{align*}
    \braket{A}{B}=\sum_{k=0}^NA^*_k\cdot B_k.
\end{align*} As the functions do not have discrete "indices" but rather continuos ones, it make sense to replace the sum with an integral, extended from $-\infty$ to $\infty$, given the functions are defined for all real numbers, we get \begin{align*}
    \braket{f(x)}{g(x)}=\int_{-\infty}^{\infty}f^*(x)g(x)dx
\end{align*} which is how the inner product is defined for functions. We now also have the ability to define orthonogality for functions. With vectors, two orthogonal vectors yield a scalar product of zero. As our inner product is supposed to be a generelized scalar product, we define two functions as being orthogonal if their inner product equals zero. In other words, functions $f(x)$ and $g(x)$ are orthogonal if, and only if, 
\begin{align*}
    \braket{f(x)}{g(x)} = \int_{-\infty}^{\infty} f^*(x)g(x)dx = 0.
\end{align*}


\subsection{Using Inner Product to Find Components}

If we have a vector $\vec{A}$ we can represent it with a basis system made up from a set of orthogonal vectors. In the most basic case, this can be a vector which exists in 3d real space with three basis vectors along the x-, y- and z-axis. Due to the basis vectors being orthogonal, taking the inner product between the vector and one of the basis vectors yields the magnitud, or norm, the the component of $\vec{A}$ that lies along that basis has. For example, if $\vec{A}=A_x\hat\imath + A_y\hat\jmath + A_z\hat k$ then $\braket{A}{\hat\imath} = A_x\cdot 1 + A_y \cdot 0 + A_z \cdot 0 = A_x$. Note that in this case we defined the basis vectors to have length 1, but if we used basis vectors of arbitrary lengt along the unit vectors, we would need to divide by the length of the basis vectors to find the component. This method for finding components of a vector given a certain basis can be used for functions as well. It was discovered by Jospeh Fourier that any bounded or periodic function could be expressed as a weighted sum of sinusodial functions with different frequencies. If we treat these sinusodial functions as basis vectors, we should be able to use the inner product to find how large of a component each frequency contributes with to build up that function, so long as all of the sinusodial functions are orthogonal to eachother. Recalling the definition of the inner product and setting up the general case for two cosine functions with two given frequencies we can do the following mathematical derivation: 

\begin{align*}
&\braket{\cos(ax)}{\cos(bx)}=\\
&=\lim_{L\rightarrow\infty}\int_{-L}^L\cos^*(ax)\cos(bx) dx\\
&=\lim_{L\rightarrow\infty} \left.\frac{\sin(ax)}{a}\cos(bx)\right\rvert^L_{-L}-\int_{-L}^L\frac{b}{a}\sin(ax)(-\sin(bx))dx\\
&= \lim_{L\rightarrow\infty} \left. \frac{1}{a}\sin(ax)\cos(bx)\right\rvert^L_{-L}+\left.\frac{b}{a}\left(\frac{-\cos(ax)}{a}\sin(bx)\right\rvert^L_{-L}-\int_{-L}^L\frac{b}{a}(-\cos(ax)\cos(bx))dx\right)\\
&= \lim_{L\rightarrow\infty}\left.\frac{1}{a}\sin(ax)\cos(bx)-\frac{b}{a^2}\cos(ax)\sin(bx)\right\rvert^L_{-L}+ \frac{b^2}{a^2}\int_{-L}^L\cos(ax)\cos(bx)dx\\
&\implies \lim_{L\rightarrow\infty}\int_{-L}^L\cos(ax)\cos(bx) dx= \\
&=\lim_{L\rightarrow\infty}\left.\frac{1}{a}\sin(ax)\cos(bx)-\frac{b}{a^2}\cos(ax)\sin(bx)\right\rvert^L_{-L}+ \frac{b^2}{a^2}\int_{-L}^L\cos(ax)\cos(bx)dx\\
&\implies \lim_{L\rightarrow\infty} \left( 1-\frac{b^2}{a^2}\right)\int_{-L}^L\cos(ax)\cos(bx) dx = \lim_{L\rightarrow\infty}\left.\frac{1}{a}\sin(ax)\cos(bx)-\frac{b}{a^2}\cos(ax)\sin(bx)\right\rvert^L_{-L}\\
 &\implies\lim_{L\rightarrow\infty}\int_{-L}^L\cos(ax)\cos(bx) dx = \lim_{L\rightarrow\infty} \left( 1-\frac{b^2}{a^2}\right)^{-1}\left.\left(  \frac{1}{a}\sin(ax)\cos(bx)-\frac{b}{a^2}\cos(ax)\sin(bx)   \right)\right\rvert^L_{-L}\\ 
\end{align*}

We now have an expression for the integral, but we have discovered that the limit does not converge. It will however stay bounded, and for our purposes, and with a bit of unrigorus handwaving, this shall suffice. The interation done is however not applicable when $a=b$ due to the denominator in our expression becoming 0. A different, far simpler solution can than be found by using the trigonemetric identity \begin{align*}
    \cos(2x)=\cos^2(x)-\sin^2(x)=2\cos^2(x)-1\\
    \implies \cos^2(x)=\frac{\cos(2x)+1}{2}
\end{align*} meaning that we can write

\begin{align*}
    &\lim_{L\rightarrow\infty}\int_{-L}^L\cos(ax)\cos(ax)dx\\
    =&\lim_{L\rightarrow\infty}\int_{-L}^L\cos^2(ax)dx\\
    =&\lim_{L\rightarrow\infty}\int_{-L}^L \frac{\cos(ax/2)+1}{2}\\
    =&\lim_{L\rightarrow\infty} \left.\frac{\sin(ax/2)}{a}+\frac{x}{2}\right\rvert_{-L}^L\\
    =&\lim_{L\rightarrow\infty} \frac{2\sin(aL/2)}{a} + L
\end{align*} which clearly diverges towards infinity. As we can see, when the frequencies are the same, the inner product goes becomes large and when they are different they stay relatively small. While this is not ideal, the problem becomes resolved if we simply divide by the inner product between the sinusodial function with itself. This means that to find the component of a function $f(x)$ along the basis of $\cos(ax)$ we write the expression 
\begin{align*}
    \frac{\braket{f(x)}{\cos(ax)}}{\braket{\cos(ax)}{\cos(ax)}}.
\end{align*} This is similiar to how we need to divide by some constant if the basis used does not have a magnitude of 1. 





