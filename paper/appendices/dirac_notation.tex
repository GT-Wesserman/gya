\section{Dirac Notation} \label{appendix:dirac}
Within quantum mechanics, vectors and lini transformations play an important role. However, as anyone familiar with liniear algebra can tell you, vectors and matricies, which represent linear transformations, can have differet apperances depending on the basis choosen. This, togheter with other needs for a conviniet way to notate vectors and transformations, can be met by using Dirac notation.

\subsection{The Bra, the ket and the inner product}
The ket is, quite simply, a basis independent way to depict a vector, and this vector may be very abstract with an arbitrary number of dimensions which do not neccessarily represent physical space. It is written as $\ket{A}$ and can be expanded as $\ket{A}=A_1\ket{\epsilon_1}+A_2\ket{\epsilon_2}+...+A_N\ket{\epsilon_N}$ where $\ket{\epsilon_i}$ is the ket representing a basis vector of your choosing. However, this is not always necessary, as the kets are do not need a defined basis, but defining one can be useful in different situations, and a new basis can always be used instead. The bra is similair to the ket in appearance, with the bra for the vector $\vec{A}$ looking like $\bra{A}$. The bra itself is however not a vector, but a vector can be "placed" in a bra in order to define it. The bra then becomes a linear functional, which simply means that has the ability to act on a vector to transform it into a scalar. As such, at the relatively simple level we are working at, the bra can be thought of as transforming a vector into a function which acts on other vectors and yields scalar outputs. To see more clearly what this "function" does, we can use column vector notation. Given a vector $\vec{A}$ we get the ket and bra from $\vec{A}$, represented by some basis vectors $\epsilon_1,\epsilon_2,...,\epsilon_N$
\begin{align*}
    \ket{A}&=\begin{pmatrix}
    A_1\\
    A_2\\
    \vdots\\
    A_N
    \end{pmatrix} \\
    \bra{A}&=\begin{pmatrix}
        A_1^* &A_2^* & \dots&A_N^*
    \end{pmatrix}.
\end{align*} By combining the bra with the ket we get the equation
\begin{align*}
    \braket{A}{A}=\begin{pmatrix}
        A_1^* &A_2^* & \dots&A_N^*
    \end{pmatrix} \begin{pmatrix}
         A_1\\
    A_2\\
    \vdots\\
    A_N
    \end{pmatrix}=|\vec{A}|^2.
\end{align*} It is from this definition easy to see that multiplying a bra with a ket is equivilent to the scalar product for real valued 2 or 3 dimensional vectors. This is used to define the inner product, which is simply a more general version of the scalar product. One way to understand what the inner product is and why it is important, we can look to what the scalar product tells us. For two vectors, the scalar product is greater when the vectors are more aligned and less when they are more perpendicular to each other, with the product being 0 when they are completely perpendicular. It is a way to map how much of one vector can be projected in the direction of another. By defining the more general inner product, it becomes possible to see how "parallel" or "perpendicular" more abstract mathematical objects are to each other, as placing objects within kets or bras is not restricted to only vectors. Functions can also be placed within them. There are a few different ways to understand this. One way is to imagine a function to represent a vector with infinetly many components where the index for a basis is any number within the functions domain and the value of the function at any given input is the magnitude of the component with the index of the input. We saw from the example with vectors, and using matrix multiplication, that the inner product of the 2 finite vectors $\vec{A}$ and $\vec{B}$ is \begin{align*}
    \braket{A}{B}=\sum_{k=0}^NA^*_k\cdot B_k.
\end{align*} As the functions do not have discrete "indices" but rather continuos ones, it make sense to replace the sum with an integral, extended from $-\infty$ to $\infty$, given the functions are defined for all real numbers, we get \begin{align*}
    \braket{f(x)}{g(x)}=\int_{-\infty}^{\infty}f^*(x)g(x)dx
\end{align*} which is how the inner product is defined for functions. We now also have the ability to define orthonogality for functions. With vectors, two orthogonal vectors yield a scalar product of zero. As our inner product is supposed to be a generelized scalar product, we define two functions as being orthogonal if their inner product equals zero. In other words, functions $f(x)$ and $g(x)$ are orthogonal if, and only if, 
\begin{align*}
    \braket{f(x)}{g(x)} = \int_{-\infty}^{\infty} f^*(x)g(x)dx = 0.
\end{align*}


\subsection{Using Inner Product to Find Components}

If we have a vector $\vec{A}$ we can represent it with a basis system made up from a set of orthogonal vectors. In the most basic case, this can be a vector which exists in 3d real space with three basis vectors along the x-, y- and z-axis. Due to the basis vectors being orthogonal, taking the inner product between the vector and one of the basis vectors yields the magnitud, or norm, the the component of $\vec{A}$ that lies along that basis has. For example, if $\vec{A}=A_x\hat\imath + A_y\hat\jmath + A_z\hat k$ then $\braket{A}{\hat\imath} = A_x\cdot 1 + A_y \cdot 0 + A_z \cdot 0 = A_x$. Note that in this case we defined the basis vectors to have length 1, but if we used basis vectors of arbitrary lengt along the unit vectors, we would need to divide by the length of the basis vectors to find the component. This method for finding components of a vector given a certain basis can be used for functions as well. It was discovered by Jospeh Fourier that any bounded or periodic function could be expressed as a weighted sum of sinusodial functions with different frequencies. If we treat these sinusodial functions as basis vectors, we should be able to use the inner product to find how large of a component each frequency contributes with to build up that function. To be more general, we will, instead of simply using sinusodial functions, use $e^{-ikx}=\cos(kx)-i\sin(kx)$. To use the inner product, we must also show that two of these expressions are orthogonal to eachother if they have different frequencies. Take for example a function $\cos(k_1x)$. We should expect that all other basis, meaning $e^{-ikx}$ where $k\not=k_1$, should give an inner product of 0 and when $k=k_1$ should be some number other than 0. We calculate the integral:

\begin{align*}
    &\lim_{L\rightarrow \infty} \int_{-L}^L \cos(k_1)e^{-ikx}dx\\
    =&\lim_{L\rightarrow \infty} \int_{-L}^L \frac{1}{2}(e^{ik_1x}+e^{-ik_1x})e^{-ikx}dx\\
    = &\lim_{L\rightarrow \infty} \frac{1}{2}\int_{-L}^L e^{i(k_1-k)x}+e^{-i(k_1+k)x}dx\\
    = &\lim_{L\rightarrow \infty} \left.\frac{1}{2} \left(     
     \frac{e^{i(k_1-k)x}}{i(k_1-k)}+\frac{e^{-i(k_1+k)x}}{-i(k_1+k)} \right)\right\rvert_{-L}^L\\
     = &\lim_{L\rightarrow \infty} \frac{1}{2i}\left( \frac{e^{i(k_1-k)L}}{(k_1-k)}- \frac{e^{-i(k_1-k)L}}{(k_1-k)} + \frac{e^{i(k_1+k)L}}{(k_1+k)}- \frac{e^{-i(k_1+k)L}}{(k_1+k)}\right)\\
     = &\lim_{L\rightarrow \infty} \frac{\sin((k_1-k)L)}{k_1-k} + \frac{\sin((k_1+k)L)}{k_1+k}.
\end{align*} While this does not converge to 0 as hoped, it will stay bounded and for our purposes will remain sufficiently small. We end up dividing by $k_1-k$, so if we let $k_1=k$ our calculation changes to 
\begin{align*}
    &\lim_{L\rightarrow \infty} \frac{1}{2}\int_{-L}^L e^{i(k_1-k)x}+e^{-i(k_1+k)x}dx\\
    =&\lim_{L\rightarrow \infty} \frac{1}{2}\int_{-L}^L 1+e^{-2ikx}dx\\
    =&\lim_{L\rightarrow \infty} \frac{1}{2}\left.\left( x+\frac{e^ {-2ikx}}{-2ik}\right)\right\rvert_{-L}^L\\
    =&\lim_{L\rightarrow \infty}\frac{1}{2}\left(2L+\frac{e^{2ikL}-e^{-2ikL}}{2ik}\right)\\
    =&\lim_{L\rightarrow \infty} L + \frac{\sin(2kL)}{2k}
\end{align*} which goes towards infinity. While this is not strictly what we were looking for, it is still promising that when we knew that the frequency $k_1$ should be the main component, taking the inner product with $e^{-ik_1x}$  yielded far larger results than when $k\not=k_1$. Using more generalized functions and the Dirac delta, a more rigorus approach can be taken, but for our purposes, it will suffice to see this difference in size with the inner product and compensate for it by trying to normalize the function. We saw earlier that for simple vectors, normalizing could be done by dividing with the square of the magnitude, which can be calculated by taking the relevant basis vectors inner product with itself. In our case, dividing the inner product $\braket{\cos(k_1x)}{e^{-ikx}}$ with $\braket{e^{-ikx}}{e^{-ikx}}$ before taking the limit of the integral towards infinity, would yield 0 for when $k\not=k_1$ and $\frac{1}{2}$ when $k=k_1$. This is due to the conjugate being taken for the bra giving
\begin{align*}
    &\braket{e^{-ikx}}{e^{-ikx}}\\
    =&\lim_{L\rightarrow\infty} \int_{-L}^Le^{ikx}e^{-ikx}dx\\
    = &\lim_{L\rightarrow\infty} \int_{-L}^L 1dx\\
    = &\lim_{L\rightarrow\infty} 2L
\end{align*} which means that 
\begin{align*}
    &\frac{\braket{\cos(k_1x)}{e^{-ikx}}}{\braket{e^{-ikx}}{e^{-ikx}}}\\
    &=\lim_{L\rightarrow\infty}\frac{L}{2L}+\frac{\sin(2kx)}{2k\cdot 2L}\\
    &=\frac{1}{2}+0.
\end{align*} The reason for this only making up one half of the component is that $e^{-ikx}$ contains an unneccessary part of $-i\sin(kx)$ which is removed by considering that $e^{ikx}$ also yields a result of $\frac{1}{2}$ and thus, by using normalized components and after checking all possible values for $k$, one would find that the function $\cos(k_1x)$ has  a component of 0 for all functions $e^{-ikx}$ except for when $k=k_1$ or $k=-k_1$ in which case we get \begin{align*}
    \cos(k_1x) = \frac{1}{2}e^{-ik_1x} + \frac{1}{2}e^{ik_1x}
\end{align*} which is exactly what we would expect.\\ \\

By using this principle on none sinusodial functions, we can find how much of a given frequency is present within it. While we have now essentially arrived at the Fourier transform, one final thing to note is that we have played a bit fast and loose with the constants. Due to the difference between frequency in Hertz and angular frequency, where they differ by a constant $2\pi$, the Fourier transform when using wavenumber $k$ actually has a constant of $\frac{1}{\sqrt{2\pi}}$ in front so that after taking the Fourier transform followed by the inverse Fourier transform, which has the same constant, one still arrives at the same original expression.      



% \begin{align*}
% &\braket{\cos(ax)}{\cos(bx)}=\\
% &=\lim_{L\rightarrow\infty}\int_{-L}^L\cos^*(ax)\cos(bx) dx\\
% &=\lim_{L\rightarrow\infty} \left.\frac{\sin(ax)}{a}\cos(bx)\right\rvert^L_{-L}-\int_{-L}^L\frac{b}{a}\sin(ax)(-\sin(bx))dx\\
% &= \lim_{L\rightarrow\infty} \left. \frac{1}{a}\sin(ax)\cos(bx)\right\rvert^L_{-L}+\left.\frac{b}{a}\left(\frac{-\cos(ax)}{a}\sin(bx)\right\rvert^L_{-L}-\int_{-L}^L\frac{b}{a}(-\cos(ax)\cos(bx))dx\right)\\
% &= \lim_{L\rightarrow\infty}\left.\frac{1}{a}\sin(ax)\cos(bx)-\frac{b}{a^2}\cos(ax)\sin(bx)\right\rvert^L_{-L}+ \frac{b^2}{a^2}\int_{-L}^L\cos(ax)\cos(bx)dx\\
% &\implies \lim_{L\rightarrow\infty}\int_{-L}^L\cos(ax)\cos(bx) dx= \\
% &=\lim_{L\rightarrow\infty}\left.\frac{1}{a}\sin(ax)\cos(bx)-\frac{b}{a^2}\cos(ax)\sin(bx)\right\rvert^L_{-L}+ \frac{b^2}{a^2}\int_{-L}^L\cos(ax)\cos(bx)dx\\
% &\implies \lim_{L\rightarrow\infty} \left( 1-\frac{b^2}{a^2}\right)\int_{-L}^L\cos(ax)\cos(bx) dx = \lim_{L\rightarrow\infty}\left.\frac{1}{a}\sin(ax)\cos(bx)-\frac{b}{a^2}\cos(ax)\sin(bx)\right\rvert^L_{-L}\\
%  &\implies\lim_{L\rightarrow\infty}\int_{-L}^L\cos(ax)\cos(bx) dx = \lim_{L\rightarrow\infty} \left( 1-\frac{b^2}{a^2}\right)^{-1}\left.\left(  \frac{1}{a}\sin(ax)\cos(bx)-\frac{b}{a^2}\cos(ax)\sin(bx)   \right)\right\rvert^L_{-L}\\ 
% \end{align*}

% We now have an expression for the integral, but we have discovered that the limit does not converge. It will however stay bounded, and for our purposes, and with a bit of unrigorus handwaving, this shall suffice. The interation done is however not applicable when $a=b$ due to the denominator in our expression becoming 0. A different, far simpler solution can than be found by using the trigonemetric identity \begin{align*}
%     \cos(2x)=\cos^2(x)-\sin^2(x)=2\cos^2(x)-1\\
%     \implies \cos^2(x)=\frac{\cos(2x)+1}{2}
% \end{align*} meaning that we can write

% \begin{align*}
%     &\lim_{L\rightarrow\infty}\int_{-L}^L\cos(ax)\cos(ax)dx\\
%     =&\lim_{L\rightarrow\infty}\int_{-L}^L\cos^2(ax)dx\\
%     =&\lim_{L\rightarrow\infty}\int_{-L}^L \frac{\cos(ax/2)+1}{2}\\
%     =&\lim_{L\rightarrow\infty} \left.\frac{\sin(ax/2)}{a}+\frac{x}{2}\right\rvert_{-L}^L\\
%     =&\lim_{L\rightarrow\infty} \frac{2\sin(aL/2)}{a} + L
% \end{align*} which clearly diverges towards infinity. As we can see, when the frequencies are the same, the inner product goes becomes large and when they are different they stay relatively small. While this is not ideal, the problem becomes resolved if we simply divide by the inner product between the sinusodial function with itself. This means that to find the component of a function $f(x)$ along the basis of $\cos(ax)$ we write the expression 
% \begin{align*}
%     \frac{\braket{f(x)}{\cos(ax)}}{\braket{\cos(ax)}{\cos(ax)}}.
% \end{align*} This is similiar to how we need to divide by some constant if the basis used does not have a magnitude of 1. 





